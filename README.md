# Thai Handwritten OCR with Vision Transformer

A deep learning project for recognizing Thai handwritten tone marks using Vision Transformer (ViT) architecture. This model can classify Thai tone marks including à¹€à¸­à¸ (Ek), à¹‚à¸— (Tho), à¸•à¸£à¸µ (Tri), and à¸ˆà¸±à¸•à¸§à¸² (Chattawa).

## ğŸ¯ Project Overview

This project implements an Optical Character Recognition (OCR) system specifically designed for Thai handwritten tone marks using state-of-the-art Transformer architecture. The model leverages Vision Transformer (ViT) from Hugging Face to achieve high accuracy in classifying Thai tone marks.

## ğŸ“Š Dataset

The dataset contains handwritten Thai tone mark images organized into 4 classes:
- **à¹€à¸­à¸01.png** - First tone mark (Mai Ek)
- **à¹‚à¸—02.png** - Second tone mark (Mai Tho) 
- **à¸•à¸£à¸µ03.png** - Third tone mark (Mai Tri)
- **à¸ˆà¸±à¸•à¸§à¸²04.png** - Fourth tone mark (Mai Chattawa)

**Dataset Statistics:**
- Total samples: 1,773 images
- Training set: 80% (1,418 images)
- Test set: 20% (355 images)
- Image size: 224x224 pixels
- Format: RGB

## ğŸ—ï¸ Model Architecture

### Vision Transformer (ViT)
- **Base Model**: `google/vit-base-patch16-224-in21k`
- **Patch Size**: 16x16
- **Input Resolution**: 224x224
- **Number of Classes**: 4 (Thai tone marks)
- **Fine-tuning**: Classifier layer adapted for Thai tone mark classification

### Model Components
```
ViT Base Model
â”œâ”€â”€ Patch Embedding (16x16 patches)
â”œâ”€â”€ Transformer Encoder (12 layers)
â”‚   â”œâ”€â”€ Multi-Head Self-Attention
â”‚   â”œâ”€â”€ MLP Feed Forward
â”‚   â””â”€â”€ Layer Normalization
â””â”€â”€ Classification Head (4 classes)
```

## ğŸš€ Getting Started

### Prerequisites
This project runs entirely in Google Colab - no local installation required!

### Google Colab Setup

[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/yourusername/thai-handwritten-ocr/blob/main/ThaiHandWrite_Transformer_Based_OCR_tones.ipynb)

#### Quick Start in Colab
1. **Open the notebook** in Google Colab using the badge above
2. **Mount Google Drive** to access your dataset:
```python
from google.colab import drive
drive.mount('/content/drive')
```

3. **Install required packages** (already included in notebook):
```python
!pip install transformers torch torchvision scikit-learn pandas tqdm
```

4. **Run all cells** to train the model automatically

#### Dataset Setup in Google Drive
```
Google Drive/
â””â”€â”€ MyDrive/
    â””â”€â”€ characters_dataset/
        â””â”€â”€ tones/
            â”œâ”€â”€ à¹€à¸­à¸01.png/
            â”œâ”€â”€ à¹‚à¸—02.png/
            â”œâ”€â”€ à¸•à¸£à¸µ03.png/
            â””â”€â”€ à¸ˆà¸±à¸•à¸§à¸²04.png/
```

#### Training in Colab
The notebook automatically handles:
- Data loading and preprocessing
- Model training with ViT
- Performance evaluation
- Results visualization
- Model checkpointing

## ğŸ“ˆ Performance

### Training Results
- **Training Accuracy**: 90.13% (Epoch 1)
- **Loss Function**: CrossEntropyLoss
- **Optimizer**: Adam (lr=1e-4)
- **Batch Size**: 32

### Model Metrics
| Metric | Score |
|--------|-------|
| Accuracy | 90.13% |
| Precision | TBD |
| Recall | TBD |
| F1-Score | TBD |

## ğŸ“ Project Structure

```
thai-handwritten-ocr/
â”œâ”€â”€ ThaiHandWrite_Transformer_Based_OCR_tones.ipynb  # Main Colab notebook
â”œâ”€â”€ data/                                            # Dataset folder (in Google Drive)
â”‚   â””â”€â”€ characters_dataset/
â”‚       â””â”€â”€ tones/
â”‚           â”œâ”€â”€ à¹€à¸­à¸01.png/                           # 462 images
â”‚           â”œâ”€â”€ à¹‚à¸—02.png/                            # 439 images  
â”‚           â”œâ”€â”€ à¸•à¸£à¸µ03.png/                           # 463 images
â”‚           â””â”€â”€ à¸ˆà¸±à¸•à¸§à¸²04.png/                         # 319 images
â”œâ”€â”€ models/                                          # Generated in Colab
â”‚   â””â”€â”€ vit_model_checkpoint_tones.pth              # Trained model weights
â”œâ”€â”€ results/                                         # Generated outputs
â”‚   â”œâ”€â”€ classification_report_epoch_5.xlsx          # Training metrics
â”‚   â”œâ”€â”€ test_classification_report.xlsx             # Test results
â”‚   â”œâ”€â”€ loss_per_epoch.png                          # Loss visualization
â”‚   â””â”€â”€ accuracy_per_epoch.png                      # Accuracy chart
â””â”€â”€ README.md                                        # This file
```

## ğŸ”§ Implementation Details

### Data Preprocessing
```python
transform = transforms.Compose([
    transforms.Resize((224, 224)),
    transforms.ToTensor(),
    transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5])
])
```

### Training Configuration
- **Epochs**: 5
- **Learning Rate**: 1e-4
- **Batch Size**: 32
- **Loss Function**: CrossEntropyLoss
- **Optimizer**: Adam
- **Device**: CUDA (if available)

## ğŸ“Š Results Visualization

The project generates several visualization outputs:
- Training loss per epoch
- Training accuracy per epoch
- Classification reports (Excel format)
- Confusion matrix

## ğŸ› ï¸ Technical Features

- **Google Colab Ready**: No local setup required - runs entirely in the cloud
- **Vision Transformer Architecture**: State-of-the-art computer vision model
- **Transfer Learning**: Fine-tuned from pre-trained ViT model
- **GPU Acceleration**: Automatic CUDA support in Colab
- **Comprehensive Evaluation**: Multiple metrics and visualizations
- **Model Checkpointing**: Automatic save and resume training
- **Google Drive Integration**: Seamless dataset and model storage

## ğŸ“ Educational Value

This project demonstrates:
- Implementation of Vision Transformers for image classification
- Thai language processing challenges
- Transfer learning techniques
- OCR system development
- Deep learning best practices

## ğŸ¤ Contributing

Contributions are welcome! To contribute to this Colab-based project:

1. Fork the repository
2. Open the notebook in Colab
3. Make your improvements to the notebook
4. Test your changes thoroughly
5. Submit a Pull Request with:
   - Clear description of changes
   - Screenshots of results (if applicable)
   - Updated performance metrics

### Development Guidelines
- **Notebook Structure**: Keep cells organized and well-commented
- **Output Management**: Clear outputs before committing to avoid large files
- **Documentation**: Update markdown cells to explain new features
- **Testing**: Ensure all cells run successfully in a fresh Colab environment



